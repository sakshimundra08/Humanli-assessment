Project Overview : This project is a website-based AI chatbot that answers user questions strictly using content extracted from a given website. It follows a Retrieval-Augmented Generation (RAG) approach to ensure responses are grounded in website data and to reduce hallucinations. The system is designed for factual, context-based question answering rather than general knowledge responses.
Architecture Explanation : The system crawls pages from the provided website and extracts meaningful text after removing HTML noise. The text is split into large overlapping chunks to preserve context, and embeddings are generated for each chunk. These embeddings are stored in a persistent Chroma vector database. When a question is asked, relevant chunks are retrieved and sent to the LLM with strict instructions to answer only from the retrieved context.
Frameworks Used : LangChain is used for text splitting and vector store integration. Streamlit provides the interactive chat-based user interface. BeautifulSoup is used for website scraping and content cleaning. ChromaDB handles vector storage, and SentenceTransformers is used for local embedding generation.
LLM Model Used and Why : LLaMA 3.1 (8B Instant) via the Groq API is used as the language model. It was chosen for its fast inference speed, strong reasoning ability, and OpenAI-compatible API. It allows cloud-based usage without requiring local GPU hosting.
Vector Database Used and Why : ChromaDB is used as the vector database because it is lightweight, easy to integrate, and well suited for small to medium RAG systems. Its persistence feature ensures indexed data remains available across app reruns.
Embedding Strategy : Embeddings are generated using the sentence-transformers/all-MiniLM-L6-v2 model. This model provides fast local embedding generation, good semantic similarity performance, and does not require API usage, making it cost-efficient.
Setup and Run Instructions : Install dependencies from requirements.txt and run the application using streamlit run app.py. For deployment on Streamlit Cloud, add the Groq API key in the Secrets section as an environment variable.
Assumptions, Limitations, and Future Improvements : The system assumes websites contain readable static text and that relevant information exists within the crawled pages. It may not work well with dynamic or JavaScript-heavy websites. Answers are limited strictly to indexed content. Future improvements include adding source citations, implementing retrieval reranking, supporting document uploads, improving crawler depth control, and adding answer confidence scoring.